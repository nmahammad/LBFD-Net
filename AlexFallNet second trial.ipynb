{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1x-lLXBo89L_KwXERvaouwSwmx27Auk1l","timestamp":1742126290232}],"gpuType":"T4","machine_shape":"hm","mount_file_id":"1xIKtkKHzRI07lJuNPpNr7AozCF2oD6dQ","authorship_tag":"ABX9TyMwCCm9FeAEpL8eXG1W7NJ6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Let's check the GPUs**"],"metadata":{"id":"jTVvb1Gci9W8"}},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2z8oSHpi8L8","executionInfo":{"status":"ok","timestamp":1742130191509,"user_tz":-60,"elapsed":121,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}},"outputId":"57a783f6-fdf9-4210-a3a1-a2e64b6c75b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Mar 16 13:03:11 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Tl3zi4-0w0l","executionInfo":{"status":"ok","timestamp":1742130195557,"user_tz":-60,"elapsed":2724,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}},"outputId":"8c552d1c-9d65-4593-febe-d8ae04fffd2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"markdown","source":["**IMPORT NECESSARY LIBRARIES**"],"metadata":{"id":"RWUiDbxnevIy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"27m522OpW0Bz"},"outputs":[],"source":["# Standard library imports\n","from datetime import datetime\n","import os\n","import random\n","import shutil\n","from pathlib import Path\n","from typing import Tuple, List, Dict, Union\n","\n","# Scientific computing and numerical operations\n","import numpy as np\n","\n","# PyTorch core modules\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","\n","# PyTorch utilities\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim.lr_scheduler import StepLR\n","from torchinfo import summary\n","\n","\n","# Computer vision and image processing\n","from PIL import Image\n","from torchvision import transforms\n","from torchvision.models import alexnet\n","from torchvision import datasets\n","\n","# Evaluation metrics\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n","\n"]},{"cell_type":"markdown","source":["**Define the constants**"],"metadata":{"id":"Do-vdVOTezN8"}},{"cell_type":"code","source":["DATASET_PATH = Path('/content/drive/MyDrive/fall_no_fall_split')"],"metadata":{"id":"ZPfC7oT4Y8lM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Constants\n","CLASS_LABELS: Dict[str, int] = {\"no_fall\": 0, \"fall\": 1}\n","\n","# Normalization options\n","\n","TRANSFORMS_DATASET_NORM = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),  # Scales pixels from [0, 255] to [0, 1]\n","    transforms.Normalize(mean=[0.5288, 0.5161, 0.4727], std=[0.2366, 0.2398, 0.2436])\n","])\n","\n"],"metadata":{"id":"eAZO9nJIW8Wm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42  # Set a fixed seed for reproducibility\n","\n","random.seed(42)  # Set seed for Python's built-in random module\n","np.random.seed(42)  # Set seed for NumPy random operations\n","torch.manual_seed(42)  # Set seed for PyTorch CPU operations\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)  # Set seed for all CUDA devices\n","\n","torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior for cuDNN\n","torch.backends.cudnn.benchmark = False  # Disable cuDNN benchmarking for consistency\n"],"metadata":{"id":"1x-39v2tfTck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define the dataset class**"],"metadata":{"id":"0u4-NlzTe4hS"}},{"cell_type":"code","source":["class BinaryFallDataset(Dataset):\n","    def __init__(self,\n","                 root_directory_path: Path,\n","                 subset: str,\n","                 image_transforms: transforms.Compose = TRANSFORMS_DATASET_NORM) -> None:\n","        \"\"\"\n","        Initialize the dataset with parameters only (no data loading).\n","\n","        Args:\n","            root_directory_path (str | Path): Path to dataset directory containing 'train', 'validation', and 'test' subfolders.\n","            subset (str, optional): Which subset to use: 'train', 'validation', or 'test'. Defaults to 'train'.\n","            image_transforms (transforms.Compose, optional): Transformations for images. Defaults to [-1, 1] normalization.\n","        \"\"\"\n","        self.root_directory_path = Path(root_directory_path)\n","        self.subset = subset  # \"train\", \"validation\", or \"test\"\n","        self.image_transforms = TRANSFORMS_DATASET_NORM\n","        self.image_file_paths: List[Path] = []\n","        self.class_labels: List[int] = []\n","        self.is_data_loaded = False\n","\n","    def load_dataset(self) -> None:\n","        \"\"\"Load image file paths and labels from the dataset directory for the selected subset.\"\"\"\n","        if self.is_data_loaded:\n","            return  # Avoid reloading\n","\n","        subset_directory = self.root_directory_path / self.subset\n","        if not subset_directory.exists():\n","            raise FileNotFoundError(f\"Subset directory '{subset_directory}' not found.\")\n","\n","        # For each class (fall, no_fall), collect all PNG files\n","        for class_name, class_label in CLASS_LABELS.items():\n","            class_directory = subset_directory / class_name\n","            if not class_directory.exists():\n","                raise FileNotFoundError(f\"Directory '{class_directory}' not found.\")\n","\n","            image_files_in_class = list(class_directory.rglob(\"*.png\"))\n","            self.image_file_paths.extend(image_files_in_class)\n","            self.class_labels.extend([class_label] * len(image_files_in_class))\n","\n","        if not self.image_file_paths:\n","            raise ValueError(\"No images found in the dataset directory!\")\n","\n","        self.is_data_loaded = True\n","\n","    def __len__(self) -> int:\n","        if not self.is_data_loaded:\n","            raise RuntimeError(\"Dataset not loaded. Call `load_dataset()` first.\")\n","        return len(self.image_file_paths)\n","\n","    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n","        \"\"\"Load and transform a single image on demand.\"\"\"\n","        if not self.is_data_loaded:\n","            raise RuntimeError(\"Dataset not loaded. Call `load_dataset()` first.\")\n","\n","        image_file_path = self.image_file_paths[index]\n","        class_label = self.class_labels[index]\n","\n","        image_data = Image.open(image_file_path).convert(\"RGB\")\n","        image_tensor = self.image_transforms(image_data) if self.image_transforms else image_data\n","\n","        return image_tensor, class_label\n","\n","    def get_data_loader(self, batch_size: int = 16, shuffle_data: bool = True) -> DataLoader:\n","        \"\"\"Return a DataLoader with minimal RAM usage.\"\"\"\n","        if not self.is_data_loaded:\n","            self.load_dataset()\n","\n","        return DataLoader(self, batch_size=batch_size, shuffle=shuffle_data, num_workers=2, pin_memory=True, prefetch_factor=2)\n"],"metadata":{"id":"B3e5t6NfXxVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define the metric calculations**"],"metadata":{"id":"jhCnhBS3e783"}},{"cell_type":"code","source":["def numpy_sigmoid(logits):\n","    return 1 / (1 + np.exp(-logits))\n"],"metadata":{"id":"sfiZwl1i0jlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MetricsCalculator:\n","    \"\"\"Computes and stores binary classification metrics for fall detection using scikit-learn.\"\"\"\n","    def __init__(self) -> None:\n","        \"\"\"Initialize lists to store predictions and ground truth labels.\"\"\"\n","        self.ground_truth: List[int] = []\n","        self.model_prediction: List[int] = []\n","\n","    def convert_logits(self, logits: Union[np.ndarray, torch.Tensor], targets: Union[np.ndarray, torch.Tensor], threshold: float = 0.5) -> None:\n","        \"\"\"\n","        Converts raw logits to binary predictions using a sigmoid threshold\n","        and updates the stored ground truth and prediction lists.\n","\n","        Args:\n","            logits (np.ndarray or torch.Tensor): Raw model outputs with shape [batch_size, 1].\n","            targets (np.ndarray or torch.Tensor): Ground truth labels with shape [batch_size, 1] or [batch_size].\n","            threshold (float): Threshold for converting probabilities to binary predictions (default: 0.5).\n","        \"\"\"\n","        # Convert PyTorch tensors to NumPy arrays if necessary\n","        if isinstance(logits, torch.Tensor):\n","            logits = logits.detach().cpu().numpy()\n","        if isinstance(targets, torch.Tensor):\n","            targets = targets.detach().cpu().numpy()\n","\n","        # Validate input shapes\n","        if logits.ndim != 2 or logits.shape[1] != 1:\n","            raise ValueError(f\"Logits must have shape [batch_size, 1], got {logits.shape}\")\n","        if targets.shape[0] != logits.shape[0]:\n","            raise ValueError(f\"Targets batch size ({targets.shape[0]}) must match logits ({logits.shape[0]})\")\n","\n","        # Apply sigmoid to convert logits to probabilities\n","        probabilities = numpy_sigmoid(logits)\n","\n","        # Threshold probabilities to obtain binary predictions\n","        predicted_labels = (probabilities >= threshold).astype(int).flatten()\n","\n","        # Flatten targets (if not already 1D) and update lists\n","        targets_list = targets.flatten().tolist()\n","        predicted_labels_list = predicted_labels.tolist()\n","\n","        self.ground_truth.extend(targets_list)\n","        self.model_prediction.extend(predicted_labels_list)\n","\n","    def precision(self) -> float:\n","        \"\"\"Compute Precision = TP / (TP + FP).\"\"\"\n","        return precision_score(self.ground_truth, self.model_prediction, zero_division=0)\n","\n","    def recall(self) -> float:\n","        \"\"\"Compute Recall = TP / (TP + FN).\"\"\"\n","        return recall_score(self.ground_truth, self.model_prediction, zero_division=0)\n","\n","    def f1_score(self) -> float:\n","        \"\"\"Compute F1 Score = 2 * (Precision * Recall) / (Precision + Recall).\"\"\"\n","        return f1_score(self.ground_truth, self.model_prediction, zero_division=0)\n","\n","    def accuracy(self) -> float:\n","        \"\"\"Compute Accuracy = (TP + TN) / Total Samples.\"\"\"\n","        return accuracy_score(self.ground_truth, self.model_prediction)\n","\n","    def confusion_matrix(self) -> np.ndarray:\n","        return confusion_matrix(self.ground_truth, self.model_prediction)\n","\n","    def reset(self) -> None:\n","        \"\"\"Reset stored predictions and ground truth labels.\"\"\"\n","        self.ground_truth = []\n","        self.model_prediction = []\n","\n","\n"],"metadata":{"id":"5AOKZS_WXT_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Define the network `**"],"metadata":{"id":"JWIAXIjKfWQj"}},{"cell_type":"code","source":["class AlexFallNet(nn.Module):\n","\n","    def __init__(self) -> None:\n","        super(AlexFallNet, self).__init__()\n","\n","        self.alexnet = alexnet(weights=None)\n","\n","        # Modify the final classifier layer for binary classification (1 logit)\n","        self.alexnet.classifier[6] = nn.Linear(4096, 1)  # Output 1 logit for BCEWithLogitsLoss\n","\n","    def forward(self, input_tensor: Tensor) -> Tensor:\n","        \"\"\"Forward pass of AlexFallNet.\n","\n","        Args:\n","            input_tensor (Tensor): Batch of images with shape [batch_size, 3, 224, 224].\n","\n","        Returns:\n","            Tensor: Model predictions (logits) with shape [batch_size, 1].\n","        \"\"\"\n","        return self.alexnet(input_tensor)"],"metadata":{"id":"GEJchFBHZZG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AlexFallNet()\n"],"metadata":{"id":"JUjnQP1iBnIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model)"],"metadata":{"id":"b7LkHOVTBx8A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742130218704,"user_tz":-60,"elapsed":11,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}},"outputId":"fa553feb-2458-4828-eafd-496be974cf91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["=================================================================\n","Layer (type:depth-idx)                   Param #\n","=================================================================\n","AlexFallNet                              --\n","├─AlexNet: 1-1                           --\n","│    └─Sequential: 2-1                   --\n","│    │    └─Conv2d: 3-1                  23,296\n","│    │    └─ReLU: 3-2                    --\n","│    │    └─MaxPool2d: 3-3               --\n","│    │    └─Conv2d: 3-4                  307,392\n","│    │    └─ReLU: 3-5                    --\n","│    │    └─MaxPool2d: 3-6               --\n","│    │    └─Conv2d: 3-7                  663,936\n","│    │    └─ReLU: 3-8                    --\n","│    │    └─Conv2d: 3-9                  884,992\n","│    │    └─ReLU: 3-10                   --\n","│    │    └─Conv2d: 3-11                 590,080\n","│    │    └─ReLU: 3-12                   --\n","│    │    └─MaxPool2d: 3-13              --\n","│    └─AdaptiveAvgPool2d: 2-2            --\n","│    └─Sequential: 2-3                   --\n","│    │    └─Dropout: 3-14                --\n","│    │    └─Linear: 3-15                 37,752,832\n","│    │    └─ReLU: 3-16                   --\n","│    │    └─Dropout: 3-17                --\n","│    │    └─Linear: 3-18                 16,781,312\n","│    │    └─ReLU: 3-19                   --\n","│    │    └─Linear: 3-20                 4,097\n","=================================================================\n","Total params: 57,007,937\n","Trainable params: 57,007,937\n","Non-trainable params: 0\n","================================================================="]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def overfit_single_batch(model, dataloader, criterion, optimizer, scheduler, device, epochs=100, patience=10):\n","    # Retrieve the batch at index 1 (the second batch)\n","    single_batch = None\n","    for index, (images, labels) in enumerate(dataloader):\n","        if index == 1:\n","            labels = labels.float().unsqueeze(1)\n","            single_batch = (images.to(device), labels.to(device))\n","            break\n","    if single_batch is None:\n","        raise RuntimeError(\"Batch with index 1 not found in the dataloader.\")\n","\n","    batch_images, batch_labels = single_batch\n","\n","    # Initialize early stopping variables\n","    best_loss = float('inf')\n","    patience_counter = 0\n","\n","    # Overfit on this single batch for the specified number of epochs\n","    for epoch in range(epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(batch_images)\n","        loss = criterion(outputs, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        print(f\"Overfit Epoch {epoch+1}/{epochs} - Loss: {loss.item():.6f}\")\n","        scheduler.step(loss)  # Adjust learning rate based on loss\n","\n","        # Early stopping check\n","        if loss.item() < best_loss:\n","            best_loss = loss.item()\n","            patience_counter = 0  # Reset counter if loss improves\n","        else:\n","            patience_counter += 1  # Increase counter if loss doesn't improve\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered after {epoch+1} epochs. Best loss: {best_loss:.6f}\")\n","            break  # Stop training\n","\n","def main():\n","    model = AlexFallNet()\n","    model.train()\n","\n","    dataset = BinaryFallDataset(root_directory_path=DATASET_PATH, subset=\"train\")\n","    dataset.load_dataset()\n","    data_loader = dataset.get_data_loader(batch_size=16)\n","\n","    criterion = nn.BCEWithLogitsLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=5e-3)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    overfit_single_batch(model, data_loader, criterion, optimizer, scheduler, device, epochs=100, patience=10)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5BDgS1uZcCf","executionInfo":{"status":"ok","timestamp":1742130267082,"user_tz":-60,"elapsed":38781,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}},"outputId":"5d1a7b07-7081-44e5-ba14-641406bc6e40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overfit Epoch 1/100 - Loss: 0.692911\n","Overfit Epoch 2/100 - Loss: 0.692397\n","Overfit Epoch 3/100 - Loss: 0.691646\n","Overfit Epoch 4/100 - Loss: 0.689328\n","Overfit Epoch 5/100 - Loss: 0.688685\n","Overfit Epoch 6/100 - Loss: 0.685221\n","Overfit Epoch 7/100 - Loss: 0.686479\n","Overfit Epoch 8/100 - Loss: 0.686312\n","Overfit Epoch 9/100 - Loss: 0.683234\n","Overfit Epoch 10/100 - Loss: 0.681149\n","Overfit Epoch 11/100 - Loss: 0.679114\n","Overfit Epoch 12/100 - Loss: 0.676875\n","Overfit Epoch 13/100 - Loss: 0.681190\n","Overfit Epoch 14/100 - Loss: 0.674584\n","Overfit Epoch 15/100 - Loss: 0.670966\n","Overfit Epoch 16/100 - Loss: 0.675755\n","Overfit Epoch 17/100 - Loss: 0.675061\n","Overfit Epoch 18/100 - Loss: 0.671239\n","Overfit Epoch 19/100 - Loss: 0.670716\n","Overfit Epoch 20/100 - Loss: 0.667627\n","Overfit Epoch 21/100 - Loss: 0.667408\n","Overfit Epoch 22/100 - Loss: 0.665872\n","Overfit Epoch 23/100 - Loss: 0.660423\n","Overfit Epoch 24/100 - Loss: 0.663181\n","Overfit Epoch 25/100 - Loss: 0.663945\n","Overfit Epoch 26/100 - Loss: 0.654618\n","Overfit Epoch 27/100 - Loss: 0.643814\n","Overfit Epoch 28/100 - Loss: 0.649992\n","Overfit Epoch 29/100 - Loss: 0.637767\n","Overfit Epoch 30/100 - Loss: 0.624030\n","Overfit Epoch 31/100 - Loss: 0.619604\n","Overfit Epoch 32/100 - Loss: 0.615456\n","Overfit Epoch 33/100 - Loss: 0.593194\n","Overfit Epoch 34/100 - Loss: 0.586249\n","Overfit Epoch 35/100 - Loss: 0.566536\n","Overfit Epoch 36/100 - Loss: 0.576221\n","Overfit Epoch 37/100 - Loss: 0.499870\n","Overfit Epoch 38/100 - Loss: 0.458360\n","Overfit Epoch 39/100 - Loss: 0.502247\n","Overfit Epoch 40/100 - Loss: 0.494994\n","Overfit Epoch 41/100 - Loss: 0.390950\n","Overfit Epoch 42/100 - Loss: 0.399515\n","Overfit Epoch 43/100 - Loss: 0.326493\n","Overfit Epoch 44/100 - Loss: 0.453636\n","Overfit Epoch 45/100 - Loss: 0.276206\n","Overfit Epoch 46/100 - Loss: 0.304477\n","Overfit Epoch 47/100 - Loss: 0.311925\n","Overfit Epoch 48/100 - Loss: 0.247803\n","Overfit Epoch 49/100 - Loss: 0.258651\n","Overfit Epoch 50/100 - Loss: 0.226012\n","Overfit Epoch 51/100 - Loss: 0.282272\n","Overfit Epoch 52/100 - Loss: 0.181116\n","Overfit Epoch 53/100 - Loss: 0.291014\n","Overfit Epoch 54/100 - Loss: 0.136438\n","Overfit Epoch 55/100 - Loss: 0.328969\n","Overfit Epoch 56/100 - Loss: 0.073222\n","Overfit Epoch 57/100 - Loss: 0.587135\n","Overfit Epoch 58/100 - Loss: 0.291247\n","Overfit Epoch 59/100 - Loss: 0.426092\n","Overfit Epoch 60/100 - Loss: 0.346655\n","Overfit Epoch 61/100 - Loss: 0.275540\n","Overfit Epoch 62/100 - Loss: 0.169482\n","Overfit Epoch 63/100 - Loss: 0.217665\n","Overfit Epoch 64/100 - Loss: 0.094006\n","Overfit Epoch 65/100 - Loss: 0.065451\n","Overfit Epoch 66/100 - Loss: 0.064170\n","Overfit Epoch 67/100 - Loss: 0.065631\n","Overfit Epoch 68/100 - Loss: 0.101341\n","Overfit Epoch 69/100 - Loss: 0.023188\n","Overfit Epoch 70/100 - Loss: 0.096819\n","Overfit Epoch 71/100 - Loss: 0.082788\n","Overfit Epoch 72/100 - Loss: 0.103578\n","Overfit Epoch 73/100 - Loss: 0.067215\n","Overfit Epoch 74/100 - Loss: 0.058346\n","Overfit Epoch 75/100 - Loss: 0.051688\n","Overfit Epoch 76/100 - Loss: 0.099089\n","Overfit Epoch 77/100 - Loss: 0.043124\n","Overfit Epoch 78/100 - Loss: 0.069507\n","Overfit Epoch 79/100 - Loss: 0.099225\n","Early stopping triggered after 79 epochs. Best loss: 0.023188\n"]}]},{"cell_type":"markdown","source":["**Setup tensorboard for monitoring**"],"metadata":{"id":"JcQ0OhTsjYoq"}},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","\n","LOG_DIRECTORY = Path(\"/content/drive/MyDrive/tensorboard_logs/training_experiment\")\n","\n","\n","# Create TensorBoard writer\n","writer = SummaryWriter(log_dir=LOG_DIRECTORY)"],"metadata":{"id":"RKMoEE-0jjiD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define training hyper parameters**"],"metadata":{"id":"V1mHBGwXiXYr"}},{"cell_type":"code","source":["# Training hyperparameters\n","BATCH_SIZE = 16\n","NUMBER_OF_EPOCHS = 100\n","LEARNING_RATE = 5e-5\n","WEIGHT_DECAY = 5e-3\n","\n","# Early stopping settings\n","EARLY_STOP_PATIENCE = 8\n","\n","# Learning rate scheduler settings\n","SCHEDULER_STEP = 4\n","SCHEDULER_GAMMA = 0.5"],"metadata":{"id":"XxZKK2TgicFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self, model, training_loader, validation_loader, loss_criterion, computing_device, metrics_calculator):\n","        \"\"\"Initialize the trainer with model, data, and settings.\"\"\"\n","        self.model = model.to(computing_device)\n","        self.training_loader = training_loader\n","        self.validation_loader = validation_loader\n","        self.loss_criterion = loss_criterion\n","        self.optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=SCHEDULER_STEP, gamma=SCHEDULER_GAMMA)\n","        self.computing_device = computing_device\n","        self.number_of_epochs = NUMBER_OF_EPOCHS\n","        self.early_stopping_patience = EARLY_STOP_PATIENCE\n","        self.tensorboard_writer = SummaryWriter()\n","        self.metrics_calculator = metrics_calculator\n","\n","        # Create directory in Google Drive to save model\n","        self.experiment_path = self.create_experiment_folder()\n","\n","    def create_experiment_folder(self):\n","        \"\"\"Creates an experiment folder in Google Drive with timestamp.\"\"\"\n","\n","        # Define base path and timestamp\n","        base_path = \"/content/drive/MyDrive/experiment\"\n","        timestamp = datetime.now().strftime(\"exp_%d_%m_%Y_%H_%M\")\n","        experiment_path = os.path.join(base_path, timestamp)\n","\n","        # Create folder if it doesn't exist\n","        os.makedirs(experiment_path, exist_ok=True)\n","        print(f\"Experiment directory created at: {experiment_path}\")\n","\n","        return experiment_path\n","\n","    def train_one_epoch(self):\n","        \"\"\"Train the model for one epoch and return only the loss.\"\"\"\n","        self.model.train()\n","        total_training_loss = 0.0\n","\n","        for images, labels in self.training_loader:\n","            images = images.to(self.computing_device)\n","            labels = labels.to(self.computing_device).float().unsqueeze(1)  # For BCEWithLogitsLoss\n","\n","            self.optimizer.zero_grad()\n","            outputs = self.model(images)\n","            loss = self.loss_criterion(outputs, labels)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            total_training_loss += loss.item() * images.size(0)\n","\n","        average_training_loss = total_training_loss / len(self.training_loader.dataset)\n","        return average_training_loss\n","\n","    def validate_one_epoch(self):\n","        \"\"\"Validate the model and return loss plus all metrics.\"\"\"\n","        self.model.eval()\n","        self.metrics_calculator.reset()\n","        total_validation_loss = 0.0\n","\n","        with torch.no_grad():\n","            for images, labels in self.validation_loader:\n","                images = images.to(self.computing_device)\n","                labels = labels.to(self.computing_device).float().unsqueeze(1)\n","                outputs = self.model(images)\n","                loss = self.loss_criterion(outputs, labels)\n","\n","                total_validation_loss += loss.item() * images.size(0)\n","                self.metrics_calculator.convert_logits(outputs.detach().cpu().numpy(), labels.detach().cpu().numpy())\n","\n","        average_validation_loss = total_validation_loss / len(self.validation_loader.dataset)\n","        validation_accuracy = self.metrics_calculator.accuracy()\n","        validation_precision = self.metrics_calculator.precision()\n","        validation_recall = self.metrics_calculator.recall()\n","        validation_f1_score = self.metrics_calculator.f1_score()\n","        return average_validation_loss, validation_accuracy, validation_precision, validation_recall, validation_f1_score\n","\n","    def train_full_cycle(self):\n","        \"\"\"Run the full training loop with early stopping.\"\"\"\n","        best_validation_loss = float('inf')\n","        epochs_without_improvement = 0\n","        print(\"device: \", computing_device)\n","\n","        for epoch in range(self.number_of_epochs):\n","            print(f\"\\n=== Epoch {epoch + 1}/{self.number_of_epochs} ===\")\n","\n","            # Train (only loss) and validate (loss + metrics)\n","            training_loss = self.train_one_epoch()\n","            validation_loss, validation_accuracy, validation_precision, validation_recall, validation_f1_score = self.validate_one_epoch()\n","            self.scheduler.step()\n","\n","            # Log to TensorBoard\n","            self.tensorboard_writer.add_scalars(\"Loss\", {\"Training\": training_loss, \"Validation\": validation_loss}, epoch)\n","            self.tensorboard_writer.add_scalar(\"Validation/Accuracy\", validation_accuracy, epoch)\n","            self.tensorboard_writer.add_scalar(\"Validation/Precision\", validation_precision, epoch)\n","            self.tensorboard_writer.add_scalar(\"Validation/Recall\", validation_recall, epoch)\n","            self.tensorboard_writer.add_scalar(\"Validation/F1 Score\", validation_f1_score, epoch)\n","\n","            # Print metrics (training loss only, full validation metrics)\n","            print(f\"Training   - Loss: {training_loss:.4f}\")\n","            print(f\"Validation - Loss: {validation_loss:.4f} | Accuracy: {validation_accuracy:.4f} | Precision: {validation_precision:.4f} | Recall: {validation_recall:.4f} | F1: {validation_f1_score:.4f}\")\n","\n","            # Early stopping and model saving based on validation loss\n","            if validation_loss < best_validation_loss:\n","                best_validation_loss = validation_loss\n","                epochs_without_improvement = 0\n","                model_save_path = os.path.join(self.experiment_path, \"best_model.pth\")\n","                torch.save(self.model.state_dict(), model_save_path)\n","                print(f\"=> Saved best model at: {model_save_path}\")\n","            else:\n","                epochs_without_improvement += 1\n","                if epochs_without_improvement >= self.early_stopping_patience:\n","                    print(\"Early stopping triggered.\")\n","                    break\n","\n","        self.tensorboard_writer.close()\n","        print(\"\\nTraining completed!\")"],"metadata":{"id":"gEn6IIggjBA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Replace with your actual dataset path\n","    computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Initialize datasets\n","    training_dataset = BinaryFallDataset(DATASET_PATH, subset=\"train\")\n","    validation_dataset = BinaryFallDataset(DATASET_PATH, subset=\"validation\")\n","    training_dataset.load_dataset()\n","    validation_dataset.load_dataset()\n","\n","    # Get data loaders\n","    training_loader = training_dataset.get_data_loader(batch_size=BATCH_SIZE, shuffle_data=True)\n","    validation_loader = validation_dataset.get_data_loader(batch_size=BATCH_SIZE, shuffle_data=True)\n","\n","    # Initialize model, criterion, and metrics\n","    model = AlexFallNet()\n","    loss_criterion = nn.BCEWithLogitsLoss()\n","    metrics_calculator = MetricsCalculator()\n","\n","    # Initialize and run trainer\n","    trainer = Trainer(\n","        model=model,\n","        training_loader=training_loader,\n","        validation_loader=validation_loader,\n","        loss_criterion=loss_criterion,\n","        computing_device=computing_device,\n","        metrics_calculator=metrics_calculator\n","    )\n","    trainer.train_full_cycle()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGz9EaISmLX4","outputId":"f9d512fa-4592-4214-8ccc-39c30fc3d1e1","executionInfo":{"status":"ok","timestamp":1742134603602,"user_tz":-60,"elapsed":4322150,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment directory created at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04\n","device:  cuda\n","\n","=== Epoch 1/100 ===\n","Training   - Loss: 0.2353\n","Validation - Loss: 0.1214 | Accuracy: 0.9592 | Precision: 0.9697 | Recall: 0.9523 | F1: 0.9609\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 2/100 ===\n","Training   - Loss: 0.0880\n","Validation - Loss: 0.0941 | Accuracy: 0.9698 | Precision: 0.9935 | Recall: 0.9488 | F1: 0.9706\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 3/100 ===\n","Training   - Loss: 0.0611\n","Validation - Loss: 0.1276 | Accuracy: 0.9734 | Precision: 0.9957 | Recall: 0.9537 | F1: 0.9742\n","\n","=== Epoch 4/100 ===\n","Training   - Loss: 0.0546\n","Validation - Loss: 0.0524 | Accuracy: 0.9814 | Precision: 0.9915 | Recall: 0.9730 | F1: 0.9822\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 5/100 ===\n","Training   - Loss: 0.0324\n","Validation - Loss: 0.0407 | Accuracy: 0.9887 | Precision: 0.9917 | Recall: 0.9869 | F1: 0.9893\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 6/100 ===\n","Training   - Loss: 0.0254\n","Validation - Loss: 0.0468 | Accuracy: 0.9851 | Precision: 0.9937 | Recall: 0.9779 | F1: 0.9857\n","\n","=== Epoch 7/100 ===\n","Training   - Loss: 0.0255\n","Validation - Loss: 0.0267 | Accuracy: 0.9934 | Precision: 0.9951 | Recall: 0.9924 | F1: 0.9938\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 8/100 ===\n","Training   - Loss: 0.0215\n","Validation - Loss: 0.0267 | Accuracy: 0.9927 | Precision: 0.9931 | Recall: 0.9931 | F1: 0.9931\n","\n","=== Epoch 9/100 ===\n","Training   - Loss: 0.0144\n","Validation - Loss: 0.0224 | Accuracy: 0.9927 | Precision: 0.9965 | Recall: 0.9896 | F1: 0.9931\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 10/100 ===\n","Training   - Loss: 0.0122\n","Validation - Loss: 0.0234 | Accuracy: 0.9913 | Precision: 0.9965 | Recall: 0.9869 | F1: 0.9917\n","\n","=== Epoch 11/100 ===\n","Training   - Loss: 0.0095\n","Validation - Loss: 0.0223 | Accuracy: 0.9931 | Precision: 0.9924 | Recall: 0.9945 | F1: 0.9934\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 12/100 ===\n","Training   - Loss: 0.0112\n","Validation - Loss: 0.0193 | Accuracy: 0.9942 | Precision: 0.9965 | Recall: 0.9924 | F1: 0.9945\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 13/100 ===\n","Training   - Loss: 0.0080\n","Validation - Loss: 0.0172 | Accuracy: 0.9949 | Precision: 0.9958 | Recall: 0.9945 | F1: 0.9952\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\n","\n","=== Epoch 14/100 ===\n","Training   - Loss: 0.0089\n","Validation - Loss: 0.0184 | Accuracy: 0.9942 | Precision: 0.9965 | Recall: 0.9924 | F1: 0.9945\n","\n","=== Epoch 15/100 ===\n","Training   - Loss: 0.0061\n","Validation - Loss: 0.0173 | Accuracy: 0.9938 | Precision: 0.9965 | Recall: 0.9917 | F1: 0.9941\n","\n","=== Epoch 16/100 ===\n","Training   - Loss: 0.0072\n","Validation - Loss: 0.0200 | Accuracy: 0.9920 | Precision: 0.9972 | Recall: 0.9876 | F1: 0.9924\n","\n","=== Epoch 17/100 ===\n","Training   - Loss: 0.0053\n","Validation - Loss: 0.0184 | Accuracy: 0.9934 | Precision: 0.9972 | Recall: 0.9903 | F1: 0.9938\n","\n","=== Epoch 18/100 ===\n","Training   - Loss: 0.0047\n","Validation - Loss: 0.0184 | Accuracy: 0.9945 | Precision: 0.9952 | Recall: 0.9945 | F1: 0.9948\n","\n","=== Epoch 19/100 ===\n","Training   - Loss: 0.0053\n","Validation - Loss: 0.0188 | Accuracy: 0.9938 | Precision: 0.9965 | Recall: 0.9917 | F1: 0.9941\n","\n","=== Epoch 20/100 ===\n","Training   - Loss: 0.0044\n","Validation - Loss: 0.0183 | Accuracy: 0.9927 | Precision: 0.9972 | Recall: 0.9889 | F1: 0.9931\n","\n","=== Epoch 21/100 ===\n","Training   - Loss: 0.0042\n","Validation - Loss: 0.0175 | Accuracy: 0.9949 | Precision: 0.9965 | Recall: 0.9938 | F1: 0.9952\n","Early stopping triggered.\n","\n","Training completed!\n"]}]},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","\n","# # Epochs\n","# epochs = list(range(1, 18))\n","\n","# # Training and Validation Loss\n","# training_loss = [0.1948, 0.0797, 0.0555, 0.0472, 0.0234, 0.0180, 0.0170, 0.0123, 0.0058, 0.0042,\n","#                  0.0053, 0.0036, 0.0034, 0.0032, 0.0015, 0.0013, 0.0013]\n","# validation_loss = [0.0995, 0.0671, 0.0531, 0.0465, 0.0312, 0.0385, 0.0267, 0.0201, 0.0174, 0.0203,\n","#                    0.0271, 0.0543, 0.0307, 0.0205, 0.0196, 0.0271, 0.0240]\n","\n","# # Plotting\n","# plt.figure(figsize=(6, 4))\n","# plt.plot(epochs, training_loss, label=\"Training Loss\", marker=\"o\")\n","# plt.plot(epochs, validation_loss, label=\"Validation Loss\", marker=\"s\")\n","\n","# # Labels and Title\n","# plt.xlabel(\"Epochs\")\n","# plt.ylabel(\"Loss\")\n","# plt.title(\"Training vs Validation Loss\")\n","# plt.legend()\n","# plt.grid(True)\n","\n","# # Show Plot\n","# plt.show()\n"],"metadata":{"id":"Mh792L-hGdTs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***THIRD EXP***"],"metadata":{"id":"5np5yLBWIh1G"}},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","#     # Replace with your actual dataset path\n","#     computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#     # Initialize datasets\n","#     training_dataset = BinaryFallDataset(DATASET_PATH, subset=\"train\")\n","#     validation_dataset = BinaryFallDataset(DATASET_PATH, subset=\"validation\")\n","#     training_dataset.load_dataset()\n","#     validation_dataset.load_dataset()\n","\n","#     # Get data loaders\n","#     training_loader = training_dataset.get_data_loader(batch_size=BATCH_SIZE, shuffle_data=True)\n","#     validation_loader = validation_dataset.get_data_loader(batch_size=BATCH_SIZE, shuffle_data=True)\n","\n","#     # Initialize model, criterion, and metrics\n","#     model = AlexFallNet()\n","#     loss_criterion = nn.BCEWithLogitsLoss()\n","#     metrics_calculator = MetricsCalculator()\n","\n","#     # Initialize and run trainer\n","#     trainer = Trainer(\n","#         model=model,\n","#         training_loader=training_loader,\n","#         validation_loader=validation_loader,\n","#         loss_criterion=loss_criterion,\n","#         computing_device=computing_device,\n","#         metrics_calculator=metrics_calculator\n","#     )\n","#     trainer.train_full_cycle()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zr751_4cIf31","executionInfo":{"status":"ok","timestamp":1741700966295,"user_tz":-60,"elapsed":5567229,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}},"outputId":"e068f276-adf1-43c0-c489-03ed7bdb576a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment directory created at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16\n","device:  cuda\n","\n","=== Epoch 1/100 ===\n","Training   - Loss: 0.2207\n","Validation - Loss: 0.1142 | Accuracy: 0.9632 | Precision: 0.9686 | Recall: 0.9613 | F1: 0.9649\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 2/100 ===\n","Training   - Loss: 0.0879\n","Validation - Loss: 0.1143 | Accuracy: 0.9621 | Precision: 0.9558 | Recall: 0.9730 | F1: 0.9644\n","\n","=== Epoch 3/100 ===\n","Training   - Loss: 0.0660\n","Validation - Loss: 0.0560 | Accuracy: 0.9825 | Precision: 0.9874 | Recall: 0.9793 | F1: 0.9833\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 4/100 ===\n","Training   - Loss: 0.0557\n","Validation - Loss: 0.0788 | Accuracy: 0.9778 | Precision: 0.9894 | Recall: 0.9682 | F1: 0.9787\n","\n","=== Epoch 5/100 ===\n","Training   - Loss: 0.0319\n","Validation - Loss: 0.0452 | Accuracy: 0.9847 | Precision: 0.9802 | Recall: 0.9910 | F1: 0.9856\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 6/100 ===\n","Training   - Loss: 0.0292\n","Validation - Loss: 0.0369 | Accuracy: 0.9891 | Precision: 0.9979 | Recall: 0.9813 | F1: 0.9895\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 7/100 ===\n","Training   - Loss: 0.0265\n","Validation - Loss: 0.0313 | Accuracy: 0.9913 | Precision: 0.9944 | Recall: 0.9889 | F1: 0.9917\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 8/100 ===\n","Training   - Loss: 0.0236\n","Validation - Loss: 0.0255 | Accuracy: 0.9916 | Precision: 0.9924 | Recall: 0.9917 | F1: 0.9920\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 9/100 ===\n","Training   - Loss: 0.0142\n","Validation - Loss: 0.0268 | Accuracy: 0.9916 | Precision: 0.9972 | Recall: 0.9869 | F1: 0.9920\n","\n","=== Epoch 10/100 ===\n","Training   - Loss: 0.0122\n","Validation - Loss: 0.0205 | Accuracy: 0.9953 | Precision: 0.9972 | Recall: 0.9938 | F1: 0.9955\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 11/100 ===\n","Training   - Loss: 0.0140\n","Validation - Loss: 0.0230 | Accuracy: 0.9913 | Precision: 0.9944 | Recall: 0.9889 | F1: 0.9917\n","\n","=== Epoch 12/100 ===\n","Training   - Loss: 0.0127\n","Validation - Loss: 0.0246 | Accuracy: 0.9931 | Precision: 0.9972 | Recall: 0.9896 | F1: 0.9934\n","\n","=== Epoch 13/100 ===\n","Training   - Loss: 0.0087\n","Validation - Loss: 0.0201 | Accuracy: 0.9942 | Precision: 0.9979 | Recall: 0.9910 | F1: 0.9944\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 14/100 ===\n","Training   - Loss: 0.0076\n","Validation - Loss: 0.0169 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 15/100 ===\n","Training   - Loss: 0.0073\n","Validation - Loss: 0.0194 | Accuracy: 0.9934 | Precision: 0.9979 | Recall: 0.9896 | F1: 0.9938\n","\n","=== Epoch 16/100 ===\n","Training   - Loss: 0.0076\n","Validation - Loss: 0.0332 | Accuracy: 0.9887 | Precision: 0.9856 | Recall: 0.9931 | F1: 0.9893\n","\n","=== Epoch 17/100 ===\n","Training   - Loss: 0.0056\n","Validation - Loss: 0.0174 | Accuracy: 0.9960 | Precision: 0.9979 | Recall: 0.9945 | F1: 0.9962\n","\n","=== Epoch 18/100 ===\n","Training   - Loss: 0.0054\n","Validation - Loss: 0.0168 | Accuracy: 0.9956 | Precision: 0.9979 | Recall: 0.9938 | F1: 0.9958\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 19/100 ===\n","Training   - Loss: 0.0047\n","Validation - Loss: 0.0168 | Accuracy: 0.9949 | Precision: 0.9972 | Recall: 0.9931 | F1: 0.9951\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 20/100 ===\n","Training   - Loss: 0.0054\n","Validation - Loss: 0.0161 | Accuracy: 0.9956 | Precision: 0.9979 | Recall: 0.9938 | F1: 0.9958\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 21/100 ===\n","Training   - Loss: 0.0045\n","Validation - Loss: 0.0179 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","\n","=== Epoch 22/100 ===\n","Training   - Loss: 0.0045\n","Validation - Loss: 0.0163 | Accuracy: 0.9956 | Precision: 0.9979 | Recall: 0.9938 | F1: 0.9958\n","\n","=== Epoch 23/100 ===\n","Training   - Loss: 0.0044\n","Validation - Loss: 0.0158 | Accuracy: 0.9967 | Precision: 0.9972 | Recall: 0.9965 | F1: 0.9969\n","=> Saved best model at: /content/drive/MyDrive/experiment/exp_11_03_2025_12_16/best_model.pth\n","\n","=== Epoch 24/100 ===\n","Training   - Loss: 0.0041\n","Validation - Loss: 0.0172 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","\n","=== Epoch 25/100 ===\n","Training   - Loss: 0.0038\n","Validation - Loss: 0.0170 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","\n","=== Epoch 26/100 ===\n","Training   - Loss: 0.0038\n","Validation - Loss: 0.0160 | Accuracy: 0.9956 | Precision: 0.9979 | Recall: 0.9938 | F1: 0.9958\n","\n","=== Epoch 27/100 ===\n","Training   - Loss: 0.0039\n","Validation - Loss: 0.0168 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","\n","=== Epoch 28/100 ===\n","Training   - Loss: 0.0038\n","Validation - Loss: 0.0163 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","\n","=== Epoch 29/100 ===\n","Training   - Loss: 0.0038\n","Validation - Loss: 0.0160 | Accuracy: 0.9953 | Precision: 0.9979 | Recall: 0.9931 | F1: 0.9955\n","\n","=== Epoch 30/100 ===\n","Training   - Loss: 0.0035\n","Validation - Loss: 0.0161 | Accuracy: 0.9960 | Precision: 0.9979 | Recall: 0.9945 | F1: 0.9962\n","\n","=== Epoch 31/100 ===\n","Training   - Loss: 0.0031\n","Validation - Loss: 0.0163 | Accuracy: 0.9956 | Precision: 0.9979 | Recall: 0.9938 | F1: 0.9958\n","Early stopping triggered.\n","\n","Training completed!\n"]}]},{"cell_type":"markdown","source":["**Evaluation and metrics on test set**"],"metadata":{"id":"mjK5cnURfhKh"}},{"cell_type":"code","source":["WEIGHTS_PATH = Path('/content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth')"],"metadata":{"id":"NtctjVcofgfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Evaluation:\n","    def __init__(self, model, evaluation_loader, loss_criterion, computing_device, metrics_calculator):\n","        \"\"\"Initialize the evaluator with model, data, and settings.\"\"\"\n","        self.model = model.to(computing_device)\n","        self.evaluation_loader = evaluation_loader\n","        self.loss_criterion = loss_criterion\n","        self.computing_device = computing_device\n","        self.metrics_calculator = metrics_calculator\n","\n","    def evaluate(self):\n","        \"\"\"Evaluate the model on the evaluation dataset and return loss, metrics, and confusion matrix.\"\"\"\n","        self.model.eval()\n","        self.metrics_calculator.reset()\n","        total_evaluation_loss = 0.0\n","\n","        with torch.no_grad():\n","            for images, labels in self.evaluation_loader:\n","                images = images.to(self.computing_device)\n","                labels = labels.to(self.computing_device).float().unsqueeze(1)  # For BCEWithLogitsLoss\n","\n","                outputs = self.model(images)\n","                loss = self.loss_criterion(outputs, labels)\n","\n","                total_evaluation_loss += loss.item() * images.size(0)\n","                self.metrics_calculator.convert_logits(outputs.detach().cpu().numpy(), labels.detach().cpu().numpy())\n","\n","        # Compute average loss and metrics\n","        average_evaluation_loss = total_evaluation_loss / len(self.evaluation_loader.dataset)\n","        evaluation_accuracy = self.metrics_calculator.accuracy()\n","        evaluation_precision = self.metrics_calculator.precision()\n","        evaluation_recall = self.metrics_calculator.recall()\n","        evaluation_f1_score = self.metrics_calculator.f1_score()\n","        evaluation_confusion_matrix = self.metrics_calculator.confusion_matrix()\n","\n","        return (\n","            average_evaluation_loss,\n","            evaluation_accuracy,\n","            evaluation_precision,\n","            evaluation_recall,\n","            evaluation_f1_score,\n","            evaluation_confusion_matrix\n","        )\n","\n","    def print_evaluation_results(self):\n","        \"\"\"Print the evaluation results in a formatted way.\"\"\"\n","        (\n","            loss,\n","            accuracy,\n","            precision,\n","            recall,\n","            f1_score,\n","            confusion_matrix\n","        ) = self.evaluate()\n","\n","        print(\"\\n=== Evaluation Results ===\")\n","        print(f\"Loss: {loss:.4f}\")\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision: {precision:.4f}\")\n","        print(f\"Recall: {recall:.4f}\")\n","        print(f\"F1 Score: {f1_score:.4f}\")\n","        print(\"\\nConfusion Matrix:\")\n","        print(\"[[TN, FP]\")\n","        print(\" [FN, TP]]\")\n","        print(f\"{confusion_matrix[0][0]:4d}  {confusion_matrix[0][1]:4d}\")\n","        print(f\"{confusion_matrix[1][0]:4d}  {confusion_matrix[1][1]:4d}\")\n","        print(\"\\nWhere:\")\n","        print(f\"TN (True Negatives)  = {confusion_matrix[0][0]:4d} : Correctly predicted negatives\")\n","        print(f\"FP (False Positives) = {confusion_matrix[0][1]:4d} : Negatives predicted as positives\")\n","        print(f\"FN (False Negatives) = {confusion_matrix[1][0]:4d} : Positives predicted as negatives\")\n","        print(f\"TP (True Positives)  = {confusion_matrix[1][1]:4d} : Correctly predicted positives\")\n","        print(\"======================\")\n"],"metadata":{"id":"z24xrWR2f52i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load trained model\n","model = AlexFallNet()\n","computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","loss_criterion = nn.BCEWithLogitsLoss()\n","metrics_calculator = MetricsCalculator()\n","\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/experiment/exp_16_03_2025_13_04/best_model.pth\", weights_only=True))\n","model.to(computing_device)\n","\n","# Test dataset and DataLoader (from BinaryFallDataset)\n","test_dataset = BinaryFallDataset(root_directory_path=Path(DATASET_PATH), subset=\"test\")\n","test_dataset.load_dataset()\n","test_loader = test_dataset.get_data_loader(batch_size=32, shuffle_data=False)\n","\n","# Create evaluator\n","evaluator = Evaluation(\n","    model=model,\n","    evaluation_loader=test_loader,\n","    loss_criterion=loss_criterion,\n","    computing_device=computing_device,\n","    metrics_calculator=metrics_calculator\n",")\n","\n","# Run evaluation and print results\n","evaluator.print_evaluation_results()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HH5vam_pjMCt","executionInfo":{"status":"ok","timestamp":1742135062571,"user_tz":-60,"elapsed":350730,"user":{"displayName":"Fear 13","userId":"14772979768791465479"}},"outputId":"ba64d1bc-c1d4-4840-b2ff-b45ccc9c09d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Evaluation Results ===\n","Loss: 0.0263\n","Accuracy: 0.9916\n","Precision: 0.9938\n","Recall: 0.9903\n","F1 Score: 0.9920\n","\n","Confusion Matrix:\n","[[TN, FP]\n"," [FN, TP]]\n","1293     9\n","  14  1433\n","\n","Where:\n","TN (True Negatives)  = 1293 : Correctly predicted negatives\n","FP (False Positives) =    9 : Negatives predicted as positives\n","FN (False Negatives) =   14 : Positives predicted as negatives\n","TP (True Positives)  = 1433 : Correctly predicted positives\n","======================\n"]}]}]}